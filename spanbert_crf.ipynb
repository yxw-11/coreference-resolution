{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spanbert_crf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsd7G-E736hc"
      },
      "source": [
        "This notebook is based on the Jonathan K. Kummerfeld's work. And this notebook runs the coreferecne resolution model described in \"SpanBERT: Improving Pre-training by Representing and Predicting Spans\" by Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy, and released here: https://github.com/mandarjoshi90/coref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDWXLl2eTTZ"
      },
      "source": [
        "#Configuration\n",
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4lM2-lwgzCW"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdQU_a2F3dkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f279fb05-02a1-4bdf-a7e0-3fdbfae2bac2"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX6RC2n8gjEK"
      },
      "source": [
        "def get_json_list(multiclass_file, software = 'doccano'):\n",
        "    with open(multiclass_file, 'r') as json_file:\n",
        "        json_list = list(json_file)\n",
        "\n",
        "    annotations_json = []\n",
        "    for line in json_list:\n",
        "        annotations_json.append(json.loads(line))\n",
        "\n",
        "    if software == 'prodigy':\n",
        "        annotations_json = prodigy_to_doccano(annotations_json)\n",
        "\n",
        "    return annotations_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMAAd01ZgoUL"
      },
      "source": [
        "file = '/content/drive/MyDrive/ieee_ner_coref/assets/data/annotations/yx_converted_abs_combine_114.jsonl'\n",
        "annotations_json = get_json_list(file, software = 'doccano')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZisuvJvGg5IB"
      },
      "source": [
        "text = []\n",
        "for i in range(len(annotations_json)):\n",
        "  text.append(annotations_json[i]['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkDbn3dwhmbm"
      },
      "source": [
        "with open(\"span_text.txt\", 'w') as f:\n",
        "        for i in text:\n",
        "            f.write(i + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shn7o9I1jH4g"
      },
      "source": [
        "data = []\n",
        "with open(\"/content/span_text.txt\", \"r\") as f:\n",
        "  for line in f.readlines():\n",
        "    line = line.strip('\\n')\n",
        "    data.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAi7k-unjX9W"
      },
      "source": [
        "#Using this function if the text is too long\n",
        "def split_text(text):\n",
        "  n_ = int(len(text)/2)\n",
        "  idx = n_\n",
        "  for i,char in enumerate (text[n_:]):\n",
        "    if char == '.':\n",
        "      idx +=i\n",
        "      p1 = text[:idx]\n",
        "      p2 = text[idx+1:]\n",
        "      break\n",
        "  return p1, p2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaY3fKsABHyY"
      },
      "source": [
        "text = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t0xFq5II6mS"
      },
      "source": [
        "text = [\n",
        "\"Firefly is an American space Western drama television series which ran from 2002-2003, created by writer and director Joss Whedon, under his Mutant Enemy Productions label.\",\n",
        "\"Whedon served as an executive producer, along with Tim Minear.\",\n",
        "\"The series is set in the year 2517, after the arrival of humans in a new star system and follows the adventures of the renegade crew of Serenity, a 'Firefly-class' spaceship.\",\n",
        "\"The ensemble cast portrays the nine characters who live on Serenity.\",\n",
        "\"Whedon pitched the show as 'nine people looking into the blackness of space and seeing nine different things.'\",\n",
        "\"The show explores the lives of a group of people, some of whom fought on the losing side of a civil war, who make a living on the fringes of society as part of the pioneer culture of their star system.\",\n",
        "\"In this future, the only two surviving superpowers, the United States and China, fused to form the central federal government, called the Alliance, resulting in the fusion of the two cultures.\",\n",
        "\"According to Whedon's vision, 'nothing will change in the future: technology will advance, but we will still have the same political, moral, and ethical problems as today.'\",\n",
        "\"Firefly premiered in the U.S. on the Fox network on September 20, 2002.\",\n",
        "\"By mid-December, Firefly had averaged 4.7 million viewers per episode and was 98th in Nielsen ratings.\",\n",
        "\"It was canceled after 11 of the 14 produced episodes were aired.\",\n",
        "\"Despite the relatively short life span of the series, it received strong sales when it was released on DVD and has large fan support campaigns.\",\n",
        "\"It won a Primetime Emmy Award in 2003 for Outstanding Special Visual Effects for a Series.\",\n",
        "\"TV Guide ranked the series at No. 5 on their 2013 list of 60 shows that were 'Cancelled Too Soon.'\",\n",
        "\"The post-airing success of the show led Whedon and Universal Pictures to produce Serenity, a 2005 film which continues from the story of the series, and the Firefly franchise expanded to other media, including comics and a role-playing game.\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vS80mS0NKeuW",
        "outputId": "8b7bd25f-2207-4db3-85aa-ec6ea22c2282"
      },
      "source": [
        "text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This paper shows how multiple aspects of highest resolution airborne InSAR data can be combined by simultaneous backward geocoding of the individual aspects. Due to slant range geometry and effects like layover and shadowing, fusing multi-aspect SAR data of urban areas is not possible in image space. Therefore the fusion process is modeled via object space, enabling the exploitation of redundant observations for each height value.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPDAV1rTOJYP"
      },
      "source": [
        "text1 = []\n",
        "for i in range(len(text)):\n",
        "  if len(text[i])<900:\n",
        "    text1.append(text[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF3meL8vq9yW"
      },
      "source": [
        "text1 = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tHWj-EN3mnG"
      },
      "source": [
        "part1,part2 = split_text(text[15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWqvC982nauf"
      },
      "source": [
        "text1.append(part1)\n",
        "text1.append(part2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tLis-2KOWRv"
      },
      "source": [
        "text = text1.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFupiQz2tqe-"
      },
      "source": [
        "specify the fine-tune data type and model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwNiLlzItwjt"
      },
      "source": [
        "genre = \"nw\"\n",
        "# The Ontonotes data for training the model contains text from several sources\n",
        "# of very different styles. You need to specify the most suitable one out of:\n",
        "# \"bc\": broadcast conversation\n",
        "# \"bn\": broadcast news\n",
        "# \"mz\": magazine\n",
        "# \"nw\": newswire\n",
        "# \"pt\": Bible text\n",
        "# \"tc\": telephone conversation\n",
        "# \"wb\": web data\n",
        "\n",
        "model_name = \"spanbert_base\"\n",
        "# The fine-tuned model to use. Options are:\n",
        "# bert_base\n",
        "# spanbert_base\n",
        "# bert_large\n",
        "# spanbert_large"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2t185C9vXgz"
      },
      "source": [
        "#Installation the repo and requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKA0roT3ve88",
        "outputId": "34357be3-54be-4991-cad2-61a60cf7300b"
      },
      "source": [
        "! git clone https://github.com/mandarjoshi90/coref.git\n",
        "%cd coref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coref'...\n",
            "remote: Enumerating objects: 734, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 734 (delta 2), reused 0 (delta 0), pack-reused 728\u001b[K\n",
            "Receiving objects: 100% (734/734), 4.17 MiB | 18.59 MiB/s, done.\n",
            "Resolving deltas: 100% (441/441), done.\n",
            "/content/coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi0ojjLYxSBa"
      },
      "source": [
        "! sed 's/MarkupSafe==1.0/MarkupSafe==1.1.1/; s/scikit-learn==0.19.1/scikit-learn==0.21/; s/scipy==1.0.0/scipy==1.6.2/' < requirements.txt > tmp\n",
        "! mv tmp requirements.txt\n",
        "\n",
        "! sed 's/.D.GLIBCXX.USE.CXX11.ABI.0//' < setup_all.sh  > tmp\n",
        "! mv tmp setup_all.sh \n",
        "! chmod u+x setup_all.sh "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJPKX8Qy2qyl"
      },
      "source": [
        "Set some environment variables. The data directory one is used by the system, the other is so we can use the model defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXjJbzFo2rRU"
      },
      "source": [
        "import os\n",
        "os.environ['data_dir'] = \".\"\n",
        "os.environ['CHOSEN_MODEL'] = model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrdoQFS-4U4e",
        "outputId": "3ddbe45c-d24f-4ffe-e118-64613be7750a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -r requirements.txt --log install-log.txt -q\n",
        "! ./setup_all.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.5.0\n",
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[K     |████████████████████████████████| 99 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 30.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 24.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 543 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 899 kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 52.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 264 kB 74.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 889 kB 45.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 126 kB 67.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 795 kB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.3 MB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 53.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 425 kB 42.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 71 kB 10.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 245 kB 48.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 9.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 194 kB 64.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 511 kB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 94 kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 488 kB 30.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 377.1 MB 7.9 kB/s \n",
            "\u001b[K     |████████████████████████████████| 748.9 MB 539 bytes/s \n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 59.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 322 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 375 kB 42.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 44.2 MB/s \n",
            "\u001b[?25h  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for h5py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for msgpack-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for psycopg2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, which is not installed.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.2.0 which is incompatible.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow-metadata 1.1.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.7.1 which is incompatible.\n",
            "tensorflow-metadata 1.1.0 requires protobuf<4,>=3.13, but you have protobuf 3.9.1 which is incompatible.\n",
            "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.4.2 which is incompatible.\n",
            "pandas 1.1.5 requires python-dateutil>=2.7.3, but you have python-dateutil 2.6.1 which is incompatible.\n",
            "kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "googleapis-common-protos 1.53.0 requires protobuf>=3.12.0, but you have protobuf 3.9.1 which is incompatible.\n",
            "google-colab 1.0.0 requires astor~=0.8.1, but you have astor 0.8.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.9.1 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 2.10 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 0.14.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires python-dateutil>=2.8.0, but you have python-dateutil 2.6.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.1.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_frq-02o7Vzb"
      },
      "source": [
        "get fine-tune model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0U8Ad5f7R7z",
        "outputId": "e36a57cf-d9ba-4a58-a746-318f20a7ee9c"
      },
      "source": [
        "! ./download_pretrained.sh $CHOSEN_MODEL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading spanbert_base\n",
            "--2021-08-02 12:04:20--  http://nlp.cs.washington.edu/pair2vec/spanbert_base.tar.gz\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1633726311 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘./spanbert_base.tar.gz’\n",
            "\n",
            "spanbert_base.tar.g 100%[===================>]   1.52G  99.5MB/s    in 23s     \n",
            "\n",
            "2021-08-02 12:04:43 (67.3 MB/s) - ‘./spanbert_base.tar.gz’ saved [1633726311/1633726311]\n",
            "\n",
            "spanbert_base/\n",
            "spanbert_base/checkpoint\n",
            "spanbert_base/model.max.ckpt.index\n",
            "spanbert_base/stdout.log\n",
            "spanbert_base/bert_config.json\n",
            "spanbert_base/vocab.txt\n",
            "spanbert_base/model.max.ckpt.data-00000-of-00001\n",
            "spanbert_base/events.out.tfevents.1561596094.learnfair1413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igOywB0RAzdr"
      },
      "source": [
        "# Data Preparation and Prediction\n",
        "\n",
        "Process the data to be in the required input format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0jLV2_sHC7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1dd307d-190a-4ed0-c875-6826534654e3"
      },
      "source": [
        "from bert import tokenization\n",
        "import json\n",
        "\n",
        "def data_convert(text):\n",
        "  data = {\n",
        "      'doc_key': genre,\n",
        "      'sentences': [[\"[CLS]\"]],\n",
        "      'speakers': [[\"[SPL]\"]],\n",
        "      'clusters': [],\n",
        "      'sentence_map': [0],\n",
        "      'subtoken_map': [0],\n",
        "  }\n",
        "\n",
        "  # Determine Max Segment\n",
        "  max_segment = None\n",
        "  for line in open('experiments.conf'):\n",
        "      if line.startswith(model_name):\n",
        "          max_segment = True\n",
        "      elif line.strip().startswith(\"max_segment_len\"):\n",
        "          if max_segment:\n",
        "              max_segment = int(line.strip().split()[-1])\n",
        "              break\n",
        "\n",
        "  tokenizer = tokenization.FullTokenizer(vocab_file=\"cased_config_vocab/vocab.txt\", do_lower_case=False)\n",
        "  subtoken_num = 0\n",
        "  sent_n  = 0\n",
        "  for sent_num, line in enumerate(text):\n",
        "      raw_tokens = line.split()\n",
        "      tokens = tokenizer.tokenize(line)\n",
        "      if len(tokens) + len(data['sentences'][-1]) >= max_segment:\n",
        "          data['sentences'][-1].append(\"[SEP]\")\n",
        "          data['sentences'].append([\"[CLS]\"])\n",
        "          data['speakers'][-1].append(\"[SPL]\")\n",
        "          data['speakers'].append([\"[SPL]\"])\n",
        "          data['sentence_map'].append(sent_num - 1)\n",
        "          data['subtoken_map'].append(subtoken_num - 1)\n",
        "          data['sentence_map'].append(sent_num)\n",
        "          data['subtoken_map'].append(subtoken_num)\n",
        "\n",
        "      ctoken = raw_tokens[0]\n",
        "      cpos = 0\n",
        "      for token in tokens:\n",
        "          data['sentences'][-1].append(token)\n",
        "          data['speakers'][-1].append(\"-\")\n",
        "          data['sentence_map'].append(sent_num)\n",
        "          data['subtoken_map'].append(subtoken_num)\n",
        "          \n",
        "          if token.startswith(\"##\"):\n",
        "              token = token[2:]\n",
        "          if len(ctoken) == len(token):\n",
        "              subtoken_num += 1\n",
        "              cpos += 1\n",
        "              if cpos < len(raw_tokens):\n",
        "                  ctoken = raw_tokens[cpos]\n",
        "          else:\n",
        "              ctoken = ctoken[len(token):]\n",
        "      sent_n = sent_num\n",
        "\n",
        "  data['sentences'][-1].append(\"[SEP]\")\n",
        "  data['speakers'][-1].append(\"[SPL]\")\n",
        "  data['sentence_map'].append(sent_n - 1)\n",
        "  data['subtoken_map'].append(subtoken_num - 1)\n",
        "\n",
        "  with open(\"sample.in.json\", 'w') as out:\n",
        "      json.dump(data, out, sort_keys=True)\n",
        "\n",
        "data_convert(text)\n",
        "\n",
        "! cat sample.in.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"clusters\": [], \"doc_key\": \"nw\", \"sentence_map\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], \"sentences\": [[\"[CLS]\", \"St\", \"##ere\", \"##o\", \"scene\", \"capture\", \"and\", \"generation\", \"is\", \"an\", \"important\", \"face\", \"##t\", \"of\", \"presence\", \"research\", \"in\", \"that\", \"stereo\", \"##scopic\", \"images\", \"have\", \"been\", \"linked\", \"to\", \"natural\", \"##ness\", \"as\", \"a\", \"component\", \"of\", \"reported\", \"presence\", \".\", \"Three\", \"-\", \"dimensional\", \"images\", \"can\", \"be\", \"captured\", \"and\", \"presented\", \"in\", \"many\", \"ways\", \",\", \"but\", \"it\", \"is\", \"rare\", \"that\", \"the\", \"most\", \"simple\", \"and\", \"\\u201c\", \"natural\", \"\\u201d\", \"method\", \"is\", \"used\", \":\", \"full\", \"or\", \"##th\", \"##ost\", \"##ere\", \"##os\", \"##copic\", \"image\", \"capture\", \"and\", \"projection\", \".\", \"This\", \"technique\", \"mimic\", \"##s\", \"as\", \"closely\", \"as\", \"possible\", \"the\", \"geometry\", \"of\", \"the\", \"human\", \"visual\", \"system\", \"and\", \"uses\", \"con\", \"##ver\", \"##gent\", \"axis\", \"stereo\", \"##graphy\", \"with\", \"the\", \"cameras\", \"separated\", \"by\", \"the\", \"human\", \"inter\", \"##oc\", \"##ular\", \"distance\", \".\", \"It\", \"si\", \"##mu\", \"##lates\", \"human\", \"viewing\", \"angles\", \",\", \"ma\", \"##gni\", \"##fication\", \",\", \"and\", \"convergence\", \"##s\", \"so\", \"that\", \"the\", \"point\", \"of\", \"zero\", \"di\", \"##sp\", \"##arity\", \"in\", \"the\", \"captured\", \"scene\", \"is\", \"reproduced\", \"without\", \"di\", \"##sp\", \"##arity\", \"in\", \"the\", \"display\", \".\", \"In\", \"a\", \"series\", \"of\", \"experiments\", \",\", \"we\", \"have\", \"used\", \"this\", \"technique\", \"to\", \"investigate\", \"body\", \"image\", \"distortion\", \"in\", \"photographic\", \"images\", \".\", \"Three\", \"ps\", \"##ych\", \"##op\", \"##hy\", \"##si\", \"##cal\", \"experiments\", \"compared\", \"size\", \",\", \"weight\", \",\", \"or\", \"shape\", \"est\", \"##imation\", \"##s\", \"(\", \"perceived\", \"waist\", \"-\", \"hip\", \"ratio\", \")\", \"in\", \"2\", \"-\", \"D\", \"and\", \"3\", \"-\", \"D\", \"images\", \"for\", \"the\", \"human\", \"form\", \"and\", \"real\", \"or\", \"virtual\", \"abstract\", \"shapes\", \"[SEP]\"], [\"[CLS]\", \"In\", \"all\", \"cases\", \",\", \"there\", \"was\", \"a\", \"relative\", \"slim\", \"##ming\", \"effect\", \"of\", \"bin\", \"##oc\", \"##ular\", \"di\", \"##sp\", \"##arity\", \".\", \"A\", \"well\", \"-\", \"known\", \"photographic\", \"distortion\", \"is\", \"the\", \"perspective\", \"flat\", \"##ten\", \"##ing\", \"effect\", \"of\", \"te\", \"##le\", \"##ph\", \"##oto\", \"lenses\", \".\", \"A\", \"fourth\", \"ps\", \"##ych\", \"##op\", \"##hy\", \"##si\", \"##cal\", \"experiment\", \"using\", \"photographic\", \"portraits\", \"taken\", \"at\", \"different\", \"distances\", \"found\", \"a\", \"fat\", \"##ten\", \"##ing\", \"effect\", \"with\", \"te\", \"##le\", \"##ph\", \"##oto\", \"lenses\", \"and\", \"a\", \"slim\", \"##ming\", \"effect\", \"with\", \"wide\", \"-\", \"angle\", \"lenses\", \".\", \"We\", \"conclude\", \"that\", \",\", \"where\", \"possible\", \",\", \"photographic\", \"inputs\", \"to\", \"the\", \"visual\", \"system\", \"should\", \"allow\", \"it\", \"to\", \"generate\", \"the\", \"c\", \"##y\", \"##c\", \"##lop\", \"##ean\", \"point\", \"of\", \"view\", \"by\", \"which\", \"we\", \"normally\", \"see\", \"the\", \"world\", \".\", \"This\", \"is\", \"best\", \"achieved\", \"by\", \"viewing\", \"images\", \"made\", \"with\", \"full\", \"or\", \"##th\", \"##ost\", \"##ere\", \"##os\", \"##copic\", \"capture\", \"and\", \"display\", \"geometry\", \".\", \"The\", \"technique\", \"can\", \"result\", \"in\", \"more\", \"-\", \"accurate\", \"est\", \"##imation\", \"##s\", \"of\", \"object\", \"shape\", \"or\", \"size\", \"and\", \"control\", \"of\", \"o\", \"##cular\", \"suppression\", \".\", \"These\", \"are\", \"assets\", \"that\", \"have\", \"particular\", \"utility\", \"in\", \"the\", \"generation\", \"of\", \"realistic\", \"virtual\", \"environments\", \".\", \"[SEP]\"]], \"speakers\": [[\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"], [\"[SPL]\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"[SPL]\"]], \"subtoken_map\": [0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 14, 15, 16, 17, 18, 19, 20, 20, 21, 22, 23, 24, 25, 26, 26, 27, 27, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 46, 46, 47, 48, 49, 49, 50, 51, 51, 51, 51, 51, 51, 52, 53, 54, 55, 55, 56, 57, 58, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 72, 72, 73, 74, 74, 75, 76, 77, 78, 79, 80, 81, 82, 82, 82, 83, 83, 84, 85, 85, 85, 86, 87, 88, 88, 89, 89, 89, 89, 90, 91, 91, 92, 93, 94, 95, 96, 97, 98, 98, 98, 99, 100, 101, 102, 103, 104, 105, 106, 106, 106, 107, 108, 109, 109, 110, 111, 112, 113, 114, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 128, 129, 129, 129, 129, 129, 129, 130, 131, 132, 132, 133, 133, 134, 135, 136, 136, 136, 137, 137, 138, 138, 138, 139, 139, 140, 141, 141, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 154, 155, 155, 156, 157, 157, 158, 159, 160, 161, 162, 162, 163, 164, 165, 165, 165, 166, 166, 166, 166, 167, 168, 168, 168, 169, 170, 171, 172, 173, 174, 174, 174, 175, 176, 177, 177, 177, 177, 178, 178, 179, 180, 181, 181, 181, 181, 181, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 192, 192, 193, 194, 195, 195, 195, 195, 196, 197, 198, 199, 199, 200, 201, 202, 202, 202, 203, 203, 204, 205, 206, 206, 207, 208, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 221, 221, 221, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 242, 242, 242, 242, 243, 244, 245, 246, 246, 247, 248, 249, 250, 251, 252, 252, 252, 253, 253, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 262, 263, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 277, 277]}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVGe7pxAGOWF"
      },
      "source": [
        "Get Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgZTTOUh7m5h",
        "outputId": "7d74d2c2-ff13-4a72-a9b3-35183e919957"
      },
      "source": [
        "! GPU=0 python predict.py $CHOSEN_MODEL sample.in.json sample.out.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0802 13:00:07.443681 140073726924672 deprecation_wrapper.py:119] From /content/coref/coref_ops.py:11: The name tf.NotDifferentiable is deprecated. Please use tf.no_gradient instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "W0802 13:00:07.537437 140073726924672 deprecation_wrapper.py:119] From /content/coref/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0802 13:00:08.940814 140073726924672 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Setting CUDA_VISIBLE_DEVICES to: 0\n",
            "Running experiment: spanbert_base\n",
            "data_dir = \"/sdb/data/new_coref\"\n",
            "model_type = \"independent\"\n",
            "max_top_antecedents = 50\n",
            "max_training_sentences = 3\n",
            "top_span_ratio = 0.4\n",
            "max_num_speakers = 20\n",
            "max_segment_len = 384\n",
            "bert_learning_rate = 2e-05\n",
            "task_learning_rate = 0.0001\n",
            "num_docs = 2802\n",
            "dropout_rate = 0.3\n",
            "ffnn_size = 3000\n",
            "ffnn_depth = 1\n",
            "num_epochs = 20\n",
            "feature_size = 20\n",
            "max_span_width = 30\n",
            "use_metadata = true\n",
            "use_features = true\n",
            "use_segment_distance = true\n",
            "model_heads = true\n",
            "coref_depth = 2\n",
            "coarse_to_fine = true\n",
            "fine_grained = true\n",
            "use_prior = true\n",
            "train_path = \"./train.english.384.jsonlines\"\n",
            "eval_path = \"./dev.english.384.jsonlines\"\n",
            "conll_eval_path = \"./dev.english.v4_gold_conll\"\n",
            "single_example = true\n",
            "genres = [\n",
            "  \"bc\"\n",
            "  \"bn\"\n",
            "  \"mz\"\n",
            "  \"nw\"\n",
            "  \"pt\"\n",
            "  \"tc\"\n",
            "  \"wb\"\n",
            "]\n",
            "eval_frequency = 1000\n",
            "report_frequency = 100\n",
            "log_root = \".\"\n",
            "adam_eps = 1e-06\n",
            "task_optimizer = \"adam\"\n",
            "bert_config_file = \"./spanbert_base/bert_config.json\"\n",
            "vocab_file = \"./spanbert_base/vocab.txt\"\n",
            "tf_checkpoint = \"./spanbert_base/model.max.ckpt\"\n",
            "init_checkpoint = \"./spanbert_base/model.max.ckpt\"\n",
            "log_dir = \"./spanbert_base\"\n",
            "W0802 13:00:09.007487 140073726924672 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0802 13:00:09.083088 140073726924672 deprecation_wrapper.py:119] From /content/coref/independent.py:48: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0802 13:00:09.089640 140073726924672 deprecation_wrapper.py:119] From /content/coref/independent.py:50: The name tf.PaddingFIFOQueue is deprecated. Please use tf.queue.PaddingFIFOQueue instead.\n",
            "\n",
            "W0802 13:00:09.092743 140073726924672 deprecation.py:323] From /content/coref/bert/modeling.py:158: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0802 13:00:09.106877 140073726924672 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:175: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0802 13:00:09.107032 140073726924672 deprecation_wrapper.py:119] From /content/coref/bert/modeling.py:175: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0802 13:00:09.169606 140073726924672 deprecation.py:506] From /content/coref/bert/modeling.py:362: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0802 13:00:09.218611 140073726924672 deprecation.py:323] From /content/coref/bert/modeling.py:676: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0802 13:00:11.806723 140073726924672 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0802 13:00:11.879436 140073726924672 deprecation.py:323] From /content/coref/independent.py:221: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0802 13:00:11.924926 140073726924672 deprecation_wrapper.py:119] From /content/coref/util.py:117: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
            "\n",
            "W0802 13:00:12.454820 140073726924672 deprecation_wrapper.py:119] From /content/coref/independent.py:60: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "**** Trainable Variables ****\n",
            "  name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "  name = span_width_embeddings:0, shape = (30, 20), *INIT_FROM_CKPT*\n",
            "  name = mention_word_attn/output_weights:0, shape = (768, 1), *INIT_FROM_CKPT*\n",
            "  name = mention_word_attn/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/hidden_weights_0:0, shape = (2324, 3000), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = mention_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = span_width_prior_embeddings:0, shape = (30, 20), *INIT_FROM_CKPT*\n",
            "  name = width_scores/hidden_weights_0:0, shape = (20, 3000), *INIT_FROM_CKPT*\n",
            "  name = width_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = width_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = width_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = genre_embeddings:0, shape = (7, 20), *INIT_FROM_CKPT*\n",
            "  name = src_projection/output_weights:0, shape = (2324, 2324), *INIT_FROM_CKPT*\n",
            "  name = src_projection/output_bias:0, shape = (2324,), *INIT_FROM_CKPT*\n",
            "  name = antecedent_distance_emb:0, shape = (10, 20), *INIT_FROM_CKPT*\n",
            "  name = output_weights:0, shape = (20, 1), *INIT_FROM_CKPT*\n",
            "  name = output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/same_speaker_emb:0, shape = (2, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/antecedent_distance_emb:0, shape = (10, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/segment_distance/segment_distance_embeddings:0, shape = (3, 20), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/hidden_weights_0:0, shape = (7052, 3000), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/hidden_bias_0:0, shape = (3000,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/output_weights:0, shape = (3000, 1), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/slow_antecedent_scores/output_bias:0, shape = (1,), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/f/output_weights:0, shape = (4648, 2324), *INIT_FROM_CKPT*\n",
            "  name = coref_layer/f/output_bias:0, shape = (2324,), *INIT_FROM_CKPT*\n",
            "W0802 13:00:13.243693 140073726924672 deprecation_wrapper.py:119] From /content/coref/independent.py:74: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0802 13:00:13.250897 140073726924672 deprecation_wrapper.py:119] From /content/coref/optimization.py:13: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0802 13:00:13.256679 140073726924672 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0802 13:00:13.276043 140073726924672 deprecation_wrapper.py:119] From /content/coref/optimization.py:64: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "bert:task 199 27\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "2021-08-02 13:00:22.876498: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-02 13:00:22.912372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:22.912956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-08-02 13:00:22.913026: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-08-02 13:00:22.916369: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-08-02 13:00:22.919013: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-08-02 13:00:22.919889: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-08-02 13:00:22.922600: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-08-02 13:00:22.924620: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-08-02 13:00:22.929342: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-08-02 13:00:22.929485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:22.930139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:22.930804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2021-08-02 13:00:22.931157: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-08-02 13:00:22.936589: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-08-02 13:00:22.936798: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563ea669640 executing computations on platform Host. Devices:\n",
            "2021-08-02 13:00:22.936828: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2021-08-02 13:00:23.145590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:23.146326: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563f0e28e00 executing computations on platform CUDA. Devices:\n",
            "2021-08-02 13:00:23.146360: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-08-02 13:00:23.146563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:23.147131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-08-02 13:00:23.147194: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-08-02 13:00:23.147234: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2021-08-02 13:00:23.147256: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2021-08-02 13:00:23.147279: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2021-08-02 13:00:23.147299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2021-08-02 13:00:23.147319: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2021-08-02 13:00:23.147340: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-08-02 13:00:23.147522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:23.148164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:23.148761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2021-08-02 13:00:23.148831: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2021-08-02 13:00:26.638805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-02 13:00:26.638860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2021-08-02 13:00:26.638873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2021-08-02 13:00:26.639099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:26.639728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-02 13:00:26.640216: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-02 13:00:26.640258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13533 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Restoring from ./spanbert_base/model.max.ckpt\n",
            "W0802 13:00:34.912238 140073726924672 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2021-08-02 13:00:43.024080: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "Decoded 1 examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VwnkoonG0xW"
      },
      "source": [
        "# Output Handling\n",
        "\n",
        "Finally, we do a little processing to get the output to have the same token indices as our input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Eo7z-d9MKT"
      },
      "source": [
        "def convert_mention(mention,output,comb_text):\n",
        "    start = output['subtoken_map'][mention[0]]\n",
        "    end = output['subtoken_map'][mention[1]] + 1\n",
        "    nmention = (start, end)\n",
        "    mtext = ''.join(' '.join(comb_text[mention[0]:mention[1]+1]).split(\" ##\"))\n",
        "    return (nmention, mtext)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHnMeGsAG1Lw"
      },
      "source": [
        "def out_put(mention_l,convert_mention):\n",
        "  output = json.load(open(\"sample.out.txt\"))\n",
        "\n",
        "  comb_text = [word for sentence in output['sentences'] for word in sentence]\n",
        "\n",
        "\n",
        "  seen = set()\n",
        "  print('Clusters:')\n",
        "  for cluster in output['predicted_clusters']:\n",
        "      mapped = []\n",
        "      for mention in cluster:\n",
        "          seen.add(tuple(mention))\n",
        "          mapped.append(convert_mention(mention,output,comb_text))\n",
        "      print(mapped, end=\",\\n\")\n",
        "\n",
        "  print('\\nMentions:')\n",
        "  for mention in output['top_spans']:\n",
        "      if tuple(mention) in seen:\n",
        "          continue\n",
        "      mention_l.append(convert_mention(mention,output,comb_text))\n",
        "      print(convert_mention(mention,output,comb_text), end=\",\\n\")\n",
        "  return mapped , mention_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "T_K8tz0CCu6I",
        "outputId": "5ddff9fc-a6bc-4b03-d830-79795d9f3f76"
      },
      "source": [
        "text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Stereo scene capture and generation is an important facet of presence research in that stereoscopic images have been linked to naturalness as a component of reported presence. Three-dimensional images can be captured and presented in many ways, but it is rare that the most simple and “natural” method is used: full orthostereoscopic image capture and projection. This technique mimics as closely as possible the geometry of the human visual system and uses convergent axis stereography with the cameras separated by the human interocular distance. It simulates human viewing angles, magnification, and convergences so that the point of zero disparity in the captured scene is reproduced without disparity in the display. In a series of experiments, we have used this technique to investigate body image distortion in photographic images. Three psychophysical experiments compared size, weight, or shape estimations (perceived waist-hip ratio) in 2-D and 3-D images for the human form and real or virtual abstract shapes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5etBsH8BDKc",
        "outputId": "fdb74494-5a00-4103-e9ee-64c4b2f34ee3"
      },
      "source": [
        "mention_l = []\n",
        "clusters,mentions = out_put(mention_l,convert_mention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clusters:\n",
            "[((42, 48), 'the most simple and “ natural ” method'), ((56, 58), 'This technique'), ((84, 85), 'It'), ((118, 120), 'this technique'), ((247, 249), 'The technique')],\n",
            "[((212, 215), 'the visual system'), ((217, 218), 'it')],\n",
            "[((219, 220), 'generate'), ((232, 233), 'This')],\n",
            "[((252, 264), 'more - accurate estimations of object shape or size and control of ocular suppression'), ((264, 265), 'These')],\n",
            "\n",
            "Mentions:\n",
            "((0, 1), '[CLS]'),\n",
            "((0, 5), 'Stereo scene capture and generation'),\n",
            "((5, 6), 'is'),\n",
            "((10, 12), 'presence research'),\n",
            "((14, 15), 'stereoscopic'),\n",
            "((14, 16), 'stereoscopic images'),\n",
            "((18, 19), 'linked'),\n",
            "((20, 27), 'naturalness as a component of reported presence'),\n",
            "((20, 27), 'naturalness as a component of reported presence .'),\n",
            "((20, 37), 'naturalness as a component of reported presence . Three - dimensional images can be captured and presented in many ways'),\n",
            "((20, 37), 'naturalness as a component of reported presence . Three - dimensional images can be captured and presented in many ways ,'),\n",
            "((25, 27), 'reported presence'),\n",
            "((27, 28), 'Three - dimensional'),\n",
            "((27, 29), 'Three - dimensional images'),\n",
            "((27, 37), 'Three - dimensional images can be captured and presented in many ways'),\n",
            "((31, 32), 'captured'),\n",
            "((33, 34), 'presented'),\n",
            "((35, 36), 'many'),\n",
            "((35, 37), 'many ways'),\n",
            "((38, 39), 'it'),\n",
            "((42, 50), 'the most simple and “ natural ” method is used'),\n",
            "((42, 56), 'the most simple and “ natural ” method is used : full orthostereoscopic image capture and projection'),\n",
            "((42, 56), 'the most simple and “ natural ” method is used : full orthostereoscopic image capture and projection .'),\n",
            "((49, 50), 'used'),\n",
            "((50, 56), 'full orthostereoscopic image capture and projection'),\n",
            "((56, 70), 'This technique mimics as closely as possible the geometry of the human visual system'),\n",
            "((58, 59), 'mimics'),\n",
            "((58, 70), 'mimics as closely as possible the geometry of the human visual system'),\n",
            "((63, 70), 'the geometry of the human visual system'),\n",
            "((66, 70), 'the human visual system'),\n",
            "((70, 72), 'and uses'),\n",
            "((71, 72), 'uses'),\n",
            "((72, 75), 'convergent axis stereography'),\n",
            "((72, 84), 'convergent axis stereography with the cameras separated by the human interocular distance'),\n",
            "((76, 78), 'the cameras'),\n",
            "((76, 84), 'the cameras separated by the human interocular distance'),\n",
            "((78, 79), 'separated'),\n",
            "((80, 84), 'the human interocular distance'),\n",
            "((84, 92), 'It simulates human viewing angles , magnification , and convergences'),\n",
            "((85, 86), 'simulates'),\n",
            "((85, 92), 'simulates human viewing angles , magnification , and convergences'),\n",
            "((86, 92), 'human viewing angles , magnification , and convergences'),\n",
            "((94, 103), 'the point of zero disparity in the captured scene'),\n",
            "((94, 110), 'the point of zero disparity in the captured scene is reproduced without disparity in the display .'),\n",
            "((94, 115), 'the point of zero disparity in the captured scene is reproduced without disparity in the display . In a series of experiments'),\n",
            "((100, 103), 'the captured scene'),\n",
            "((104, 105), 'reproduced'),\n",
            "((108, 110), 'the display'),\n",
            "((108, 110), 'the display .'),\n",
            "((109, 110), '.'),\n",
            "((111, 115), 'a series of experiments'),\n",
            "((114, 132), ', we have used this technique to investigate body image distortion in photographic images . Three psychophysical experiments compared'),\n",
            "((115, 116), 'we'),\n",
            "((115, 118), 'we have used'),\n",
            "((115, 131), 'we have used this technique to investigate body image distortion in photographic images . Three psychophysical experiments'),\n",
            "((115, 132), 'we have used this technique to investigate body image distortion in photographic images . Three psychophysical experiments compared'),\n",
            "((116, 118), 'have used'),\n",
            "((117, 118), 'used'),\n",
            "((118, 128), 'this technique to investigate body image distortion in photographic images'),\n",
            "((118, 131), 'this technique to investigate body image distortion in photographic images . Three psychophysical experiments'),\n",
            "((121, 122), 'investigate'),\n",
            "((122, 128), 'body image distortion in photographic images'),\n",
            "((126, 128), 'photographic images'),\n",
            "((128, 131), 'Three psychophysical experiments'),\n",
            "((131, 132), 'compared'),\n",
            "((132, 140), 'size , weight , or shape estimations ( perceived waist - hip ratio )'),\n",
            "((132, 145), 'size , weight , or shape estimations ( perceived waist - hip ratio ) in 2 - D and 3 - D images'),\n",
            "((132, 149), 'size , weight , or shape estimations ( perceived waist - hip ratio ) in 2 - D and 3 - D images for the human form'),\n",
            "((141, 142), '2 - D'),\n",
            "((141, 145), '2 - D and 3 - D images'),\n",
            "((141, 142), 'D'),\n",
            "((143, 144), '3 - D'),\n",
            "((146, 149), 'the human form'),\n",
            "((150, 155), 'real or virtual abstract shapes'),\n",
            "((150, 155), 'real or virtual abstract shapes [SEP]'),\n",
            "((154, 155), '[SEP]'),\n",
            "((156, 158), 'all cases'),\n",
            "((158, 159), 'there'),\n",
            "((160, 167), 'a relative slimming effect of binocular disparity'),\n",
            "((165, 167), 'binocular disparity'),\n",
            "((167, 168), 'A'),\n",
            "((167, 169), 'A well - known'),\n",
            "((167, 171), 'A well - known photographic distortion'),\n",
            "((171, 172), 'is'),\n",
            "((172, 179), 'the perspective flattening effect of telephoto lenses'),\n",
            "((172, 190), 'the perspective flattening effect of telephoto lenses . A fourth psychophysical experiment using photographic portraits taken at different distances'),\n",
            "((177, 178), 'telephoto'),\n",
            "((177, 179), 'telephoto lenses'),\n",
            "((178, 179), '.'),\n",
            "((179, 190), 'A fourth psychophysical experiment using photographic portraits taken at different distances'),\n",
            "((184, 185), 'photographic'),\n",
            "((184, 186), 'photographic portraits'),\n",
            "((184, 187), 'photographic portraits taken'),\n",
            "((184, 188), 'photographic portraits taken at'),\n",
            "((184, 189), 'photographic portraits taken at different'),\n",
            "((184, 190), 'photographic portraits taken at different distances'),\n",
            "((186, 187), 'taken'),\n",
            "((190, 191), 'found'),\n",
            "((190, 206), 'found a fattening effect with telephoto lenses and a slimming effect with wide - angle lenses . We conclude'),\n",
            "((191, 194), 'a fattening effect'),\n",
            "((191, 197), 'a fattening effect with telephoto lenses'),\n",
            "((191, 204), 'a fattening effect with telephoto lenses and a slimming effect with wide - angle lenses'),\n",
            "((191, 206), 'a fattening effect with telephoto lenses and a slimming effect with wide - angle lenses . We conclude'),\n",
            "((195, 196), 'telephoto'),\n",
            "((195, 197), 'telephoto lenses'),\n",
            "((198, 201), 'a slimming effect'),\n",
            "((198, 204), 'a slimming effect with wide - angle lenses'),\n",
            "((202, 203), 'wide - angle'),\n",
            "((202, 204), 'wide - angle lenses'),\n",
            "((203, 206), '. We conclude'),\n",
            "((204, 205), 'We'),\n",
            "((204, 206), 'We conclude'),\n",
            "((205, 206), 'conclude'),\n",
            "((209, 215), 'photographic inputs to the visual system'),\n",
            "((209, 216), 'photographic inputs to the visual system should'),\n",
            "((209, 232), 'photographic inputs to the visual system should allow it to generate the cyclopean point of view by which we normally see the world'),\n",
            "((216, 217), 'allow'),\n",
            "((217, 220), 'it to generate'),\n",
            "((217, 232), 'it to generate the cyclopean point of view by which we normally see the world'),\n",
            "((218, 220), 'to generate'),\n",
            "((220, 232), 'the cyclopean point of view by which we normally see the world'),\n",
            "((227, 228), 'we'),\n",
            "((227, 230), 'we normally see'),\n",
            "((227, 232), 'we normally see the world'),\n",
            "((228, 230), 'normally see'),\n",
            "((229, 230), 'see'),\n",
            "((230, 232), 'the world'),\n",
            "((232, 247), 'This is best achieved by viewing images made with full orthostereoscopic capture and display geometry'),\n",
            "((237, 238), 'viewing'),\n",
            "((237, 247), 'viewing images made with full orthostereoscopic capture and display geometry'),\n",
            "((241, 247), 'full orthostereoscopic capture and display geometry'),\n",
            "((247, 251), 'The technique can result'),\n",
            "((247, 264), 'The technique can result in more - accurate estimations of object shape or size and control of ocular suppression'),\n",
            "((249, 251), 'can result'),\n",
            "((250, 251), 'result'),\n",
            "((255, 259), 'object shape or size'),\n",
            "((258, 259), 'size'),\n",
            "((260, 264), 'control of ocular suppression'),\n",
            "((269, 278), 'particular utility in the generation of realistic virtual environments .'),\n",
            "((272, 274), 'the generation'),\n",
            "((272, 278), 'the generation of realistic virtual environments'),\n",
            "((272, 278), 'the generation of realistic virtual environments .'),\n",
            "((275, 278), 'realistic virtual environments'),\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}